{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf0hFP6EyOD"
      },
      "source": [
        "#1 - Implementing the model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THda5hUrx8zJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "79XVIfb2Kz-U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from matplotlib.colors import ListedColormap\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_colab = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fceFH79Cx8T"
      },
      "source": [
        "Loading images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPpLBVfY7C8l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNDCPZ0igT02"
      },
      "outputs": [],
      "source": [
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "print(\"Checking existence \",os.chdir(r'/content/drive/MyDrive'))\n",
        "\n",
        "array_path = r'/content/drive/MyDrive/carseg_data/complete_arrays'\n",
        "val_array_path = r'/content/drive/MyDrive/carseg_data/val_arrays'\n",
        "#CMAP for our segmentation according to the colours provided\n",
        "car_cmap = ListedColormap([\n",
        "(255,255,255),\n",
        "(250, 149, 10), (19, 98, 19), (249, 249, 10),\n",
        "(10, 248, 250), (149, 7, 149), (5, 249, 9),\n",
        "(20, 19, 249), (249, 9, 250),\n",
        "(0,0,0)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5azY0anspVT"
      },
      "source": [
        "##2 - Defining the Dataset and making DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "18-KYKynBtSO"
      },
      "outputs": [],
      "source": [
        "class NumpyArrayDataset(Dataset):\n",
        "    def __init__(self, array_dir):\n",
        "        self.array_dir = array_dir\n",
        "        self.array_files = os.listdir(array_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.array_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        array_filename = self.array_files[idx]\n",
        "        array_path = os.path.join(self.array_dir, array_filename)\n",
        "\n",
        "        # Load the numpy array\n",
        "        numpy_array = np.load(array_path)\n",
        "\n",
        "        # Split the array into RGB and segmentation channels\n",
        "        rgb_channels = numpy_array[:, :, :3]  # Extract RGB channels\n",
        "        segm_channel = numpy_array[:, :, 3]   # Extract segmentation channel\n",
        "\n",
        "        # Convert to PyTorch tensors using transpose and assigning required data types\n",
        "        rgb_channels_tensor = torch.from_numpy(rgb_channels.transpose(2, 0, 1)).float()\n",
        "        segm_channel_tensor = torch.from_numpy(segm_channel).long()\n",
        "\n",
        "        # Return a dictionary containing the RGB and segmentation channels\n",
        "        return {'car': rgb_channels_tensor, 'segm_mask': segm_channel_tensor}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b0ZOhZ4VBxPN"
      },
      "outputs": [],
      "source": [
        "#Define batch size\n",
        "batch_size = 3\n",
        "\n",
        "#Create an instance of the NumpyArrayDataset\n",
        "numpy_dataset = NumpyArrayDataset(array_path)\n",
        "val_dataset = NumpyArrayDataset(val_array_path)\n",
        "#Split into training and test datasets\n",
        "train_dataset_np, val_dataset_np = train_test_split(numpy_dataset, train_size=0.75, random_state=42)\n",
        "\n",
        "# Create data loaders for the datasets\n",
        "train_dl_np = DataLoader(train_dataset_np, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validation_dl_np = DataLoader(val_dataset_np, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_dl_np = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9nQLTl8uVHh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Display only the first 3 images and segmentation masks\n",
        "num_images_to_display = 5\n",
        "fig, axs = plt.subplots(2, num_images_to_display, figsize=(16, 8))\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Get data from the dataset\n",
        "    data = numpy_dataset[i+ 700*i] # Shuffle manually so we can see different images, as the dataset itself does not shuffle them, the dataloaders do\n",
        "\n",
        "    # Extract image and segmentation mask data from the dataset\n",
        "    image_data = data['car'].numpy().transpose(1, 2, 0)\n",
        "    segmentation_mask = data['segm_mask'].numpy()\n",
        "\n",
        "    # Display original image\n",
        "    axs[0, i].set_title(f'Image {i+1}')\n",
        "    axs[0, i].imshow(image_data.astype(np.uint8))\n",
        "\n",
        "    # Display segmentation mask using the provided colormap\n",
        "    axs[1, i].set_title(f'Segmentation Mask {i+1}')\n",
        "    axs[1, i].imshow(segmentation_mask, cmap=car_cmap)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCJCt0sYxv5F"
      },
      "source": [
        "# 3 - U-Net implenetation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-f32NPMIfQkQ"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False, activation=None ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels),nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class TransposeConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, flag, kernel_size=4, stride=2, padding=1, bias=False):\n",
        "        super(TransposeConvLayer, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        ]\n",
        "        if flag == 1:\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UnetBlock(nn.Module):\n",
        "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False, innermost=False, outermost=False):\n",
        "        super(UnetBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if input_c is None:\n",
        "            input_c = nf\n",
        "        downconv = ConvLayer(input_c, ni)\n",
        "        downnorm = nn.BatchNorm2d(ni)\n",
        "        uprelu = nn.ReLU(inplace=True)\n",
        "        upnorm = nn.BatchNorm2d(nf)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = TransposeConvLayer(ni * 2, nf, flag = 1)\n",
        "            down = [downconv]\n",
        "            up = [upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = TransposeConvLayer(ni, nf, bias=False, flag = 0)\n",
        "            down = [downconv]\n",
        "            up = [upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = TransposeConvLayer(ni * 2, nf, bias=False, flag = 0)\n",
        "            down = [downconv, downnorm]\n",
        "            up = [upconv, upnorm]\n",
        "            if dropout:\n",
        "                up += [nn.Dropout(0.2)] #tweak this\n",
        "            model = down + [submodule] + up\n",
        "        self.model = nn.Sequential(*model)\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(input_c, nf, kernel_size=1, stride=1),\n",
        "            nn.BatchNorm2d(nf)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            output = self.model(x)\n",
        "            return output\n",
        "        else:\n",
        "            x_clone = x.clone()\n",
        "            residual_output = self.residual(x_clone)\n",
        "            model_output = self.model(x)\n",
        "            concatenated_output = torch.cat([residual_output, model_output], 1)\n",
        "            return concatenated_output\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, input_c=3, output_c=10, n_down=8, num_filters=64):\n",
        "        super(Unet, self).__init__()\n",
        "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
        "        for _ in range(3):\n",
        "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
        "        out_filters = num_filters * 8\n",
        "        for _ in range(3):\n",
        "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
        "            out_filters //= 2\n",
        "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return  out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSzxrWaUFMpx"
      },
      "source": [
        "## 4 - Weight initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "GEWRHtnffcj0"
      },
      "outputs": [],
      "source": [
        "def init_weights(net, init='kaiming', gain=1.414, print_message=True):\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
        "            if init == 'norm':\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "            elif init == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        if 'BatchNorm2d' in classname:\n",
        "            nn.init.normal_(m.weight.data, 1., gain)\n",
        "            nn.init.constant_(m.bias.data, 0.)\n",
        "\n",
        "    net.apply(init_func)\n",
        "    if print_message:\n",
        "        print(f\"model initialized with {init} initialization\")\n",
        "    return net\n",
        "\n",
        "def init_model(model, device, init_type='kaiming', gain=1.414):\n",
        "    model = model.to(device)\n",
        "    model = init_weights(model, init=init_type, gain=gain)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChI-4vDuY1UP"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## 5 - Losses for intermediate testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            focal_loss = self.alpha * focal_loss\n",
        "\n",
        "        return torch.mean(focal_loss)"
      ],
      "metadata": {
        "id": "WuH77HtAOa-2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self,num_classes):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, pred, label):\n",
        "      pred = F.softmax(pred, dim=1)\n",
        "      pred = torch.argmax(pred, dim=1).squeeze(1)\n",
        "      iou_list = list()\n",
        "      present_dice_list = list()\n",
        "      pred = pred.view(-1)\n",
        "      label = label.view(-1)\n",
        "      for sem_class in range(self.num_classes):\n",
        "          pred_inds = (pred == sem_class)\n",
        "          target_inds = (label == sem_class)\n",
        "          if target_inds.long().sum().item() == 0:\n",
        "              iou_now = float('nan')\n",
        "          else:\n",
        "              intersection_now = 2 * (pred_inds[target_inds]).long().sum().item()\n",
        "              union_now = pred_inds.long().sum().item() + target_inds.long().sum().item()\n",
        "              iou_now = float(intersection_now) / float(union_now)\n",
        "              present_dice_list.append(iou_now)\n",
        "          iou_list.append(iou_now)\n",
        "      return np.mean(present_dice_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "GT4rynx8uRRb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IOU_Loss(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(IOU_Loss, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def forward(self, pred, label):\n",
        "      pred = F.softmax(pred, dim=1)\n",
        "      pred = torch.argmax(pred, dim=1).squeeze(1)\n",
        "      iou_list = list()\n",
        "      present_iou_list = list()\n",
        "      pred = pred.view(-1)\n",
        "      label = label.view(-1)\n",
        "      for sem_class in range(self.num_classes):\n",
        "          pred_inds = (pred == sem_class)\n",
        "          target_inds = (label == sem_class)\n",
        "          if target_inds.long().sum().item() == 0:\n",
        "              iou_now = float('nan')\n",
        "          else:\n",
        "              intersection_now = (pred_inds[target_inds]).long().sum().item()\n",
        "              union_now = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection_now\n",
        "              iou_now = float(intersection_now) / float(union_now)\n",
        "              present_iou_list.append(iou_now)\n",
        "          iou_list.append(iou_now)\n",
        "      return np.mean(present_iou_list)\n"
      ],
      "metadata": {
        "id": "dbIY4R8OxlhJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjNXlo2IY5ZB"
      },
      "source": [
        "## 6 - Main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JZNr5r4O4Hcj"
      },
      "outputs": [],
      "source": [
        "class MainModel(nn.Module):\n",
        "    def __init__(self, net_G=None, lr_G=0.00001,\n",
        "                 beta1=0.5, beta2=0.999, lambda_L1=1):\n",
        "        super().__init__()\n",
        "        self.lr = lr_G\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.lambda_L1 = lambda_L1\n",
        "\n",
        "        if net_G is None:\n",
        "            self.net_G = init_model(Unet(input_c=3, output_c=10, n_down=8, num_filters=64), self.device)\n",
        "        else:\n",
        "            self.net_G = net_G.to(self.device)\n",
        "\n",
        "        self.opt_G = torch.optim.SGD(self.net_G.parameters(), lr=self.lr, momentum=0.9)\n",
        "        print(self.opt_G.param_groups[-1]['lr'])\n",
        "        #self.opt_G = optim.Adam(self.net_G.parameters(), lr=self.lr, betas=(beta1, beta2))\n",
        "\n",
        "        #self.LossFunction = nn.CrossEntropyLoss()  # Change loss function for segmentation\n",
        "        self.LossFunction2 = FocalLoss(gamma = 2, alpha = 0.25)\n",
        "        self.LossFunction3 = IOU_Loss(num_classes = 10)\n",
        "    def set_requires_grad(self, requires_grad=True):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    def setup_input(self, data):\n",
        "        self.car = data['car'].to(self.device)\n",
        "        self.segm_mask = data['segm_mask'].to(self.device)\n",
        "\n",
        "    def forward(self):\n",
        "\n",
        "        self.predicted_segm_mask = self.net_G(self.car)\n",
        "\n",
        "    def backward_G(self):\n",
        "\n",
        "        self.segm_mask = self.segm_mask / 10\n",
        "        self.focal_loss = self.LossFunction2(self.predicted_segm_mask.type(torch.FloatTensor),self.segm_mask.type(torch.LongTensor))\n",
        "        self.iou_loss = self.LossFunction3(self.predicted_segm_mask.type(torch.FloatTensor),self.segm_mask.type(torch.LongTensor))\n",
        "        self.focal_loss.backward()\n",
        "\n",
        "    def optimize(self):\n",
        "        self.forward()\n",
        "        self.opt_G.zero_grad()\n",
        "        self.net_G.train()\n",
        "        self.backward_G()\n",
        "        self.opt_G.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D24sa7RFxVD"
      },
      "source": [
        "The MeasureClass class is a utility class for computing and tracking the average of a value over multiple iterations. It keeps track of the count, sum, and average of the values. The reset method resets the meter, while the update method updates the meter with a new value and count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmddzknaYqx2"
      },
      "source": [
        "## 7 - MeasureClass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pyk5I7x_fis8"
      },
      "outputs": [],
      "source": [
        "class MeasureClass:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.count, self.avg, self.sum = [0.] * 3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += count * val\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def create_loss_meters():\n",
        "    focal_loss = MeasureClass()\n",
        "    seg_loss = MeasureClass()\n",
        "    iou_loss = MeasureClass()\n",
        "    return {\n",
        "            'focal_loss': focal_loss,\n",
        "            'iou_loss': iou_loss}\n",
        "\n",
        "def update_losses(model, loss_meter_dict, count, writer, step):\n",
        "    for loss_name, loss_meter in loss_meter_dict.items():\n",
        "        loss = getattr(model, loss_name)\n",
        "        loss_meter.update(loss.item(), count=count)\n",
        "        writer.add_scalar(loss_name, loss_meter.avg, step)\n",
        "\n",
        "def log_results(loss_meter_dict, step, writer, num_epoch):\n",
        "    for loss_name, loss_meter in loss_meter_dict.items():\n",
        "        l_avg = loss_meter.avg\n",
        "        #print(f\"{loss_name}: {l_avg:.5f}\")\n",
        "        writer.add_scalar(loss_name, loss_meter.avg, step)\n",
        "\n",
        "def plot_results(loss_list, val_loss_list, num_epochs):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), loss_list, label='Focal Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_loss_list[0], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Total Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkYvEOfWy3K2"
      },
      "source": [
        "## 8 - Training the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader,writer,step):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss_meter = create_loss_meters()  # Create loss meter for validation\n",
        "    l1=[]\n",
        "    l3=[]\n",
        "    for data in val_loader:\n",
        "        model.setup_input(data)\n",
        "        model.forward()\n",
        "        update_losses(model, val_loss_meter, count=data['car'].size(0),writer = writer,step = step)\n",
        "    for loss_name, loss_meter in val_loss_meter.items():\n",
        "                l_avg = loss_meter.avg\n",
        "                if loss_name == \"focal_loss\":\n",
        "                  l1.append(l_avg)\n",
        "                if loss_name == \"iou_loss\":\n",
        "                  l3.append(l_avg)\n",
        "    vl1 = sum(l1)/len(l1)\n",
        "    vl3 = sum(l3)/len(l3)\n",
        "    return (vl1,vl3)"
      ],
      "metadata": {
        "id": "Q8ARGKQdHrS8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puTVQulTfklb"
      },
      "outputs": [],
      "source": [
        "loss_list = [] #For plotting the loss\n",
        "val_loss_list = [[],[]]\n",
        "def train_model(model, train_loader, epochs):\n",
        "    writer = SummaryWriter()\n",
        "    step = 0\n",
        "    total_losses = []\n",
        "    best_loss = 100\n",
        "    counter = 0\n",
        "    lr_ctr = 0\n",
        "    ctr_mult = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        print('Epoch ', epoch)\n",
        "        loss_meter_dict = create_loss_meters()\n",
        "        i = 0\n",
        "        avg_loss_list = []\n",
        "        for data in tqdm(train_loader):\n",
        "            model.set_requires_grad()\n",
        "            model.setup_input(data)\n",
        "            model.optimize()\n",
        "            update_losses(model, loss_meter_dict, count=data['car'].size(0), writer=writer, step=step)\n",
        "            i += 1\n",
        "            step += 1\n",
        "            for loss_name, loss_meter in loss_meter_dict.items():\n",
        "                l_avg = loss_meter.avg\n",
        "                if loss_name == \"focal_loss\":\n",
        "                  avg_loss_list.append(l_avg)\n",
        "            log_results(loss_meter_dict, step, writer, epoch)\n",
        "\n",
        "        loss_epoch = sum(avg_loss_list)/len(avg_loss_list)\n",
        "\n",
        "        loss_list.append(loss_epoch)\n",
        "\n",
        "        (a,c) = validate_model(model, validation_dl_np, writer,step)  # Define function to calculate validation loss\n",
        "        print(f'Validation Losses (focal, iou): {a,c}')\n",
        "        val_loss_list[0].append(a)\n",
        "        val_loss_list[1].append(c)\n",
        "\n",
        "        print('Epoch ', epoch)\n",
        "        lr_ctr+=1\n",
        "        patience = 3\n",
        "        print(loss_epoch, best_loss)\n",
        "        if (epoch%10 == 0) and epoch > 50:\n",
        "          torch.save(model.state_dict(), os.path.join(r'/content/drive/MyDrive/', 'epoch-{}.pt'.format(epoch)))\n",
        "        if loss_epoch < best_loss - 0.02:\n",
        "            best_loss = loss_epoch\n",
        "            print(best_loss, loss_epoch)\n",
        "            counter = 0  # Reset counter\n",
        "        else:\n",
        "            counter += 1\n",
        "            ctr_mult += 1\n",
        "\n",
        "        if counter >= patience:\n",
        "          if model.lr*5 < 0.1:\n",
        "            model.lr = model.lr*5\n",
        "            model.opt_G = torch.optim.SGD(model.parameters(), lr=model.lr, momentum=0.9)\n",
        "            print(f'mult learning rate ', model.lr)\n",
        "          counter = 0\n",
        "\n",
        "        if lr_ctr == 5:\n",
        "          lr_ctr = 0\n",
        "          model.lr = model.lr/2\n",
        "          model.opt_G = torch.optim.SGD(model.parameters(), lr=model.lr, momentum=0.9)\n",
        "          print('div lr ', model.lr)\n",
        "        plot_results(loss_list, val_loss_list, epoch)\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "model = MainModel()\n",
        "num_epochs = 30\n",
        "train_model(model, train_dl_np, num_epochs)\n",
        "\n",
        "plot_results(loss_list, val_loss_list, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(r'/content/drive/MyDrive/', 'model_trained.pt'))"
      ],
      "metadata": {
        "id": "Tj3N02nCWcJ3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEEY8gC13Ge5"
      },
      "source": [
        "## 9 - Visualisation of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC1qlcV1vqSh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_segmentation(valid_dl, num_samples=5):\n",
        "    model1 = MainModel()\n",
        "    path = \"/content/drive/MyDrive/epoch-60.pt\"\n",
        "    model1.load_state_dict(torch.load(path))\n",
        "    model1.eval()\n",
        "    vis_iterator = iter(valid_dl)\n",
        "    with torch.no_grad():\n",
        "      for j in range(15):\n",
        "        data = next(vis_iterator)\n",
        "        for i in range(2):\n",
        "            car_scans = data['car'][i].cpu().numpy()\n",
        "            model1.setup_input(data)\n",
        "            # Forward pass\n",
        "            model1.forward()\n",
        "            # Get predicted segmentation\n",
        "            segms = data['segm_mask'][i].cpu().numpy()\n",
        "            predicted_masks = model1.predicted_segm_mask[i].cpu().numpy()\n",
        "            print('IOU metric: ', IOU_Loss(num_classes = 10)(model1.predicted_segm_mask.type(torch.FloatTensor),model1.segm_mask.type(torch.LongTensor)))\n",
        "            print('Dice metric: ', DiceLoss(num_classes = 10)(model1.predicted_segm_mask.type(torch.FloatTensor),model1.segm_mask.type(torch.LongTensor)))\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            # Display the original car image\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.title('Car image')\n",
        "            plt.imshow(car_scans.transpose(1,2,0).astype(np.uint8))\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Display the actual segmentation\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.title('Actual segmentation')\n",
        "            plt.imshow(segms,cmap=car_cmap)\n",
        "            plt.colorbar()\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Display the predicted segmentation mask\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.title('Predicted segmentation')\n",
        "            predicted_masks = np.argmax(predicted_masks,axis = 0)*10\n",
        "            plt.imshow(predicted_masks,cmap=car_cmap)\n",
        "            plt.axis('on')\n",
        "            plt.colorbar()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "visualize_segmentation(train_dl_np, num_samples=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nqAVLr7QBFt"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
